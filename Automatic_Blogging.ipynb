{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBONMwTz9y18DdV5jAoIBb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/advik-7/Blogging-Automation/blob/main/Automatic_Blogging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Dependencies**\n"
      ],
      "metadata": {
        "id": "ay7AR3j6Pf3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph -qq"
      ],
      "metadata": {
        "id": "4y3GL9chZtLv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e4238e2-0f14-4765-a338-13302a4a480a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/145.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.8/145.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install serpapi -qq"
      ],
      "metadata": {
        "id": "mIdLScvanrbQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community -qq\n",
        "!pip install pypdf -qq"
      ],
      "metadata": {
        "id": "eQz5xIzl18f0",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f682a737-0efb-42e0-e4b4-2673586e1f08"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m1.9/2.5 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/412.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.4/412.4 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.7/298.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-search-results -qq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJfELILxoA5H",
        "outputId": "bf33de39-d505-4472-c646-b34c464c6266"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-serp-api -qq"
      ],
      "metadata": {
        "id": "a2itDn48oUmx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade google-generativeai -qq"
      ],
      "metadata": {
        "id": "eImf3Tmx0-lG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu -qq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NN4KeReKP91p",
        "outputId": "878f6ffb-cff1-45f9-a61a-0cbaec0b9caa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_google_genai -qq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbmhFH2ZQ1RJ",
        "outputId": "cb90058c-3dbe-44bc-e9c8-4e21cd1a9815"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from typing import Optional, List, Mapping, Any\n",
        "from langchain_core.tools import tool\n",
        "from langchain.llms.base import LLM\n",
        "import os\n",
        "from pydantic import BaseModel\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.graph import StateGraph\n",
        "from langchain.schema import AIMessage\n"
      ],
      "metadata": {
        "id": "GU4HYCYKVTSf"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from serpapi.google_search import GoogleSearch"
      ],
      "metadata": {
        "id": "wDe_gR55r8xe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Getting the Recent Trends**"
      ],
      "metadata": {
        "id": "yNJrNPX0Pk53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_related_queries(keyword, api_key):\n",
        "    \"\"\"\n",
        "    Fetch related queries for a given keyword using SerpApi Google Trends engine.\n",
        "\n",
        "    Args:\n",
        "        keyword (str): The keyword to search trends for.\n",
        "        api_key (str): Your SerpApi API key.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with 'rising' and 'top' related queries.\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"engine\": \"google_trends\",\n",
        "        \"q\": keyword,\n",
        "        \"data_type\": \"RELATED_QUERIES\",\n",
        "        \"api_key\": api_key,\n",
        "    }\n",
        "\n",
        "    search = GoogleSearch(params)\n",
        "    results = search.get_dict()\n",
        "\n",
        "    if \"related_queries\" in results:\n",
        "        related_queries = results[\"related_queries\"]\n",
        "        rising = [\n",
        "            {\"query\": item[\"query\"], \"value\": item[\"value\"], \"link\": item[\"link\"]}\n",
        "            for item in related_queries.get(\"rising\", [])\n",
        "        ]\n",
        "        top = [\n",
        "            {\"query\": item[\"query\"], \"value\": item[\"value\"], \"link\": item[\"link\"]}\n",
        "            for item in related_queries.get(\"top\", [])\n",
        "        ]\n",
        "\n",
        "        return {\"rising\": rising, \"top\": top}\n",
        "    else:\n",
        "        return {\"error\": \"No related queries found or an issue occurred.\"}\n",
        "\n"
      ],
      "metadata": {
        "id": "PQuYSgIAopfi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Replace with your API key\n",
        "keyword = \"applied AI\"\n",
        "queries = get_related_queries(keyword, api_key)\n",
        "print(\"Rising Queries:\")\n",
        "for query in queries[\"rising\"]:\n",
        "  print(f\"- {query['query']} (Growth: {query['value']})\")\n",
        "\n",
        "  print(\"\\nTop Queries:\")\n",
        "for query in queries[\"top\"]:\n",
        "    print(f\"- {query['query']} (Interest: {query['value']})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRmo8oG4r_c5",
        "outputId": "21278271-ea18-41b3-b5bb-8e86599a7366"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rising Queries:\n",
            "- ai humanizer (Growth: Breakout)\n",
            "\n",
            "Top Queries:\n",
            "- applied digital corp (Growth: Breakout)\n",
            "\n",
            "Top Queries:\n",
            "- nvidia stock price (Growth: Breakout)\n",
            "\n",
            "Top Queries:\n",
            "- outlier ai (Growth: Breakout)\n",
            "\n",
            "Top Queries:\n",
            "- which ai technology is commonly used to convert spoken language into text on devices like smartphones? (Growth: Breakout)\n",
            "\n",
            "Top Queries:\n",
            "- intel stock (Growth: Breakout)\n",
            "\n",
            "Top Queries:\n",
            "- big bear ai stock (Growth: Breakout)\n",
            "\n",
            "Top Queries:\n",
            "- palantir stock (Growth: Breakout)\n",
            "\n",
            "Top Queries:\n",
            "- serve robotics stock (Growth: Breakout)\n",
            "\n",
            "Top Queries:\n",
            "- recursion pharmaceuticals stock (Growth: Breakout)\n",
            "\n",
            "Top Queries:\n",
            "- sound hound ai stock (Growth: Breakout)\n",
            "\n",
            "Top Queries:\n",
            "- devin ai (Growth: Breakout)\n",
            "\n",
            "Top Queries:\n",
            "- gemini (Growth: +2,400%)\n",
            "\n",
            "Top Queries:\n",
            "- gemini ai (Growth: +1,950%)\n",
            "\n",
            "Top Queries:\n",
            "- applied digital (Growth: +80%)\n",
            "\n",
            "Top Queries:\n",
            "- what is ai (Interest: 100) [Link: https://trends.google.com/trends/explore?q=what+is+ai&date=today+12-m]\n",
            "- what is applied ai (Interest: 99) [Link: https://trends.google.com/trends/explore?q=what+is+applied+ai&date=today+12-m]\n",
            "- applied science (Interest: 64) [Link: https://trends.google.com/trends/explore?q=applied+science&date=today+12-m]\n",
            "- generative ai (Interest: 51) [Link: https://trends.google.com/trends/explore?q=generative+ai&date=today+12-m]\n",
            "- applied intelligence (Interest: 47) [Link: https://trends.google.com/trends/explore?q=applied+intelligence&date=today+12-m]\n",
            "- machine learning (Interest: 45) [Link: https://trends.google.com/trends/explore?q=machine+learning&date=today+12-m]\n",
            "- applied research (Interest: 45) [Link: https://trends.google.com/trends/explore?q=applied+research&date=today+12-m]\n",
            "- google ai (Interest: 42) [Link: https://trends.google.com/trends/explore?q=google+ai&date=today+12-m]\n",
            "- applied digital (Interest: 40) [Link: https://trends.google.com/trends/explore?q=applied+digital&date=today+12-m]\n",
            "- applied ai course (Interest: 39) [Link: https://trends.google.com/trends/explore?q=applied+ai+course&date=today+12-m]\n",
            "- applied artificial intelligence (Interest: 36) [Link: https://trends.google.com/trends/explore?q=applied+artificial+intelligence&date=today+12-m]\n",
            "- artificial intelligence (Interest: 36) [Link: https://trends.google.com/trends/explore?q=artificial+intelligence&date=today+12-m]\n",
            "- applied sciences (Interest: 31) [Link: https://trends.google.com/trends/explore?q=applied+sciences&date=today+12-m]\n",
            "- chatgpt ai (Interest: 28) [Link: https://trends.google.com/trends/explore?q=chatgpt+ai&date=today+12-m]\n",
            "- chat ai (Interest: 23) [Link: https://trends.google.com/trends/explore?q=chat+ai&date=today+12-m]\n",
            "- applied materials (Interest: 20) [Link: https://trends.google.com/trends/explore?q=applied+materials&date=today+12-m]\n",
            "- deep ai (Interest: 17) [Link: https://trends.google.com/trends/explore?q=deep+ai&date=today+12-m]\n",
            "- gemini (Interest: 17) [Link: https://trends.google.com/trends/explore?q=gemini&date=today+12-m]\n",
            "- open ai (Interest: 16) [Link: https://trends.google.com/trends/explore?q=open+ai&date=today+12-m]\n",
            "- gemini ai (Interest: 16) [Link: https://trends.google.com/trends/explore?q=gemini+ai&date=today+12-m]\n",
            "- applied scientist (Interest: 15) [Link: https://trends.google.com/trends/explore?q=applied+scientist&date=today+12-m]\n",
            "- ai humanizer (Interest: 2) [Link: https://trends.google.com/trends/explore?q=ai+humanizer&date=today+12-m]\n",
            "- applied digital corp (Interest: 2) [Link: https://trends.google.com/trends/explore?q=applied+digital+corp&date=today+12-m]\n",
            "- nvidia stock price (Interest: 1) [Link: https://trends.google.com/trends/explore?q=nvidia+stock+price&date=today+12-m]\n",
            "- outlier ai (Interest: 1) [Link: https://trends.google.com/trends/explore?q=outlier+ai&date=today+12-m]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_breakout_queries(queries):\n",
        "    # Extract breakout queries from both \"rising\" and \"top\" sections\n",
        "    breakout_queries = []\n",
        "\n",
        "    # Check the 'rising' section\n",
        "    for query in queries.get(\"rising\", []):\n",
        "        if query.get(\"value\") == \"Breakout\":\n",
        "            breakout_queries.append(query)\n",
        "\n",
        "    # Check the 'top' section\n",
        "    for query in queries.get(\"top\", []):\n",
        "        if query.get(\"value\") == \"Breakout\":\n",
        "            breakout_queries.append(query)\n",
        "\n",
        "    return breakout_queries\n",
        "\n",
        "def choose_best_topic(queries):\n",
        "    # Select the topic with the highest interest or growth\n",
        "    best_topic = None\n",
        "    best_value = -float('inf')\n",
        "\n",
        "    for query in queries:\n",
        "        value = query.get('value', '')\n",
        "        # If it's a numeric growth value, try to convert it to a number\n",
        "        if isinstance(value, str) and value != \"Breakout\":\n",
        "            try:\n",
        "                # Convert growth percentage string to a numeric value (remove % sign and commas)\n",
        "                growth_value = float(value.replace('%', '').replace(',', ''))\n",
        "                if growth_value > best_value:\n",
        "                    best_value = growth_value\n",
        "                    best_topic = query\n",
        "            except ValueError:\n",
        "                continue\n",
        "        elif value == \"Breakout\" and best_topic is None:\n",
        "            # If there's no topic yet, set the first Breakout as the best\n",
        "            best_topic = query\n",
        "\n",
        "    return best_topic\n",
        "\n",
        "# Get breakout queries\n",
        "breakout_queries = get_breakout_queries(queries)\n",
        "\n",
        "# Choose the best topic (with the highest importance, based on growth or interest)\n",
        "best_topic = choose_best_topic(breakout_queries)\n",
        "print(best_topic['query'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVxbhzDkNbM3",
        "outputId": "48294cc7-efb5-47ff-8cd5-566e8e7cd99d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ai humanizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Generating Basic Blog Structure**"
      ],
      "metadata": {
        "id": "gf-nlNvUQDeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import Tool, initialize_agent, AgentType\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain_core.tools import tool\n",
        "from langchain.llms.base import LLM\n",
        "import requests\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from typing import Optional, List, Mapping, Any\n",
        "import faiss\n",
        "\n",
        "\n",
        "@tool\n",
        "def retrieve_relevant_chunks(query, pdf_file_path, model_name, top_k=5):\n",
        "    \"\"\"Generalized function to retrieve relevant chunks using FAISS from a PDF document.\"\"\"\n",
        "    # Extract text as plain text from all pages\n",
        "    loader = PyPDFLoader(pdf_file_path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Combine all document texts into a single string\n",
        "    pdf_text = \"\\n\".join([doc.page_content for doc in documents])\n",
        "\n",
        "    # Split the text into chunks (you can fine-tune this logic based on your needs)\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "    chunks = text_splitter.split_text(pdf_text)\n",
        "\n",
        "    # Assuming FAISS index is named after the PDF file (same directory as the PDF)\n",
        "    faiss_index_path = pdf_file_path.replace('.pdf', '_faiss_index.bin')\n",
        "    index = faiss.read_index(faiss_index_path)\n",
        "\n",
        "    # Create the SentenceTransformer model to encode the query\n",
        "    model = SentenceTransformer(model_name)\n",
        "    query_embedding = model.encode([query]).astype('float32')\n",
        "\n",
        "    # Perform FAISS search to get the relevant chunks\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    # Return the top-k relevant chunks\n",
        "    return [chunks[i].strip() for i in indices[0]]\n",
        "\n",
        "@tool\n",
        "def query_llm(prompt: str, context: str = \"\") -> str:\n",
        "    \"\"\"LLM query tool for response generation using Gemini.\"\"\"\n",
        "    genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "    complete_prompt = f\"{context}\\n\\nQuestion: {prompt}\\nAnswer:\"\n",
        "    response = model.generate_content([complete_prompt])\n",
        "    return response.text\n",
        "@tool\n",
        "def generate_blog_structure_with_insights(query: str, pdf_file_path_1: str, pdf_file_path_2: str) -> str:\n",
        "    \"\"\"Generate the structure of the blog based on the query and insights from provided PDFs, focusing on the blog layout and sections.\"\"\"\n",
        "\n",
        "    # Retrieve insights on writing blogs from two different PDF sources\n",
        "    insights_1 = retrieve_relevant_chunks(query, pdf_file_path_1, \"sentence-transformers/LaBSE\", top_k=3)\n",
        "    insights_2 = retrieve_relevant_chunks(query, pdf_file_path_2, \"all-MiniLM-L6-v2\", top_k=3)\n",
        "\n",
        "    # Combine insights from both PDFs\n",
        "    combined_insights = \"\\n\".join(insights_1 + insights_2)\n",
        "\n",
        "    # Generate blog structure using these insights\n",
        "    blog_structure_prompt = f\"\"\"\n",
        "    You are a content strategist, and your task is to create a **structure** for a blog based on the following query. Your focus is **creating the framework**—not writing content—but organizing the blog into sections that are relevant and effective.\n",
        "\n",
        "    Use the insights below to shape the **layout and structure** for the blog. The blog should be broken down into clear, actionable sections.\n",
        "\n",
        "    **Insights:**\n",
        "    {combined_insights}\n",
        "\n",
        "    **Blog Topic:** {query}\n",
        "\n",
        "    The blog should include the following sections:\n",
        "    1. **A catchy title**\n",
        "    2. **An introductory paragraph**—an overview of the blog topic\n",
        "    3. **Subheadings**—key sections or topics that the blog will cover\n",
        "    4. **Suggestions for images, logos, or media**—where to place visuals to enhance the blog\n",
        "    5. **A conclusion**—what should the reader take away from the blog?\n",
        "\n",
        "    Please provide only the **structure** for the blog, not the actual content. Focus on organizing the sections effectively and leaving room for creative content creation later.\n",
        "    \"\"\"\n",
        "\n",
        "    # Use the wrapped LLM to generate the blog structure\n",
        "    return wrapped_llm(blog_structure_prompt)\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Main Index Retrieval\",\n",
        "        func=retrieve_relevant_chunks,\n",
        "        args=[\"query\", \"pdf_file_path_1\", \"sentence-transformers/LaBSE\"],  # specify required arguments\n",
        "        description=\"Retrieve relevant chunks from the main pdf about insights on how to write blogs. The purpose is to assist in structuring the blog framework.\",\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Side Index Retrieval\",\n",
        "        func=retrieve_relevant_chunks,\n",
        "        args=[\"query\", \"pdf_file_path_2\", \"all-MiniLM-L6-v2\"],  # specify required arguments\n",
        "        description=\"Retrieve relevant chunks from the side pdf about insights on how to write blogs. This helps in building the blog layout.\",\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"LLM Query\",\n",
        "        func=query_llm,\n",
        "        description=\"Query the LLM with context and prompt. This helps in guiding the agent's understanding of the query and structure.\",\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Blog Structure Generator with Insights\",\n",
        "        func=generate_blog_structure_with_insights,\n",
        "        description=\"Generate the structure for a blog based on the query for a trending topic using insights from PDFs. The focus is on laying out the blog’s sections, not writing content.\",\n",
        "    )\n",
        "]\n",
        "\n",
        "\n",
        "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "\n",
        "class CustomGemini:\n",
        "    \"\"\"Custom class to interact with Google Gemini.\"\"\"\n",
        "\n",
        "    def __init__(self, temperature: float, max_tokens: int, model: str, google_api_key: str):\n",
        "        self.temperature = temperature\n",
        "        self.max_tokens = max_tokens\n",
        "        self.model = model\n",
        "        genai.configure(api_key=google_api_key)\n",
        "        self.generation_config = {\n",
        "            \"temperature\": temperature,\n",
        "            \"max_output_tokens\": max_tokens,\n",
        "        }\n",
        "        self.model_instance = genai.GenerativeModel(\n",
        "            model_name=model,\n",
        "            generation_config=self.generation_config,\n",
        "        )\n",
        "\n",
        "    def __call__(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        response = self.model_instance.generate_content([prompt])\n",
        "        return response.text\n",
        "\n",
        "custom_gemini = CustomGemini(\n",
        "    temperature=0.7,\n",
        "    max_tokens=512,\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=os.environ[\"GEMINI_API_KEY\"]\n",
        ")\n",
        "\n",
        "\n",
        "class CustomLLMWrapper(LLM):\n",
        "    custom_llm: CustomGemini\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom_gemini\"\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        return self.custom_llm(prompt, stop=stop)\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        return {\n",
        "            \"temperature\": self.custom_llm.temperature,\n",
        "            \"max_tokens\": self.custom_llm.max_tokens,\n",
        "            \"model\": self.custom_llm.model,\n",
        "        }\n",
        "\n",
        "wrapped_llm = CustomLLMWrapper(custom_llm=custom_gemini)\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=wrapped_llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=False,\n",
        "    handle_parsing_errors=True,\n",
        "    max_iterations=5,\n",
        ")\n"
      ],
      "metadata": {
        "id": "msx9QBa00CyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cac6515-a9e0-44bb-f41e-dcd23c18f5cd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-bb1b43bddd65>:163: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
            "  agent = initialize_agent(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = best_topic['query']\n",
        "pdf_file_path_1 = \"/content/CMI_Ultimate-Blogging-final.pdf\"\n",
        "pdf_file_path_2 = \"/content/ImpulseCreative-BeginnersGuide-BusinessBlogging-2020.pdf\"\n",
        "\n",
        "response = agent.run(input=query, pdf_file_path_1=pdf_file_path_1, pdf_file_path_2=pdf_file_path_2)\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "aE0x_5u8y8yH",
        "outputId": "cd777488-3042-4082-d453-c6d05fd783e2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-c4748525a577>:5: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = agent.run(input=query, pdf_file_path_1=pdf_file_path_1, pdf_file_path_2=pdf_file_path_2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The final answer is the blog structure provided by the Blog Structure Generator, as detailed in the Observation section above.  This structure includes an introduction defining \"AI Humanizer,\" a body divided into sections focusing on AI as an assistant, AI mimicking human behavior, AI as a bridge, and AI augmenting human capabilities, and a conclusion summarizing the key points and offering a future outlook.  Each section includes suggested headlines and content ideas, emphasizing the use of data, examples, and narrative techniques to engage the reader.  The structure is designed to be informative and thought-provoking, exploring the multifaceted nature of AI's impact on humanity.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Generating a Blog**"
      ],
      "metadata": {
        "id": "xpcGRXGZRWeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class BlogState(BaseModel):\n",
        "    topic: str\n",
        "    structure: str  # Added structure field\n",
        "    quotes: str = \"\"\n",
        "    blog_content: str = \"\"\n",
        "    final_blog: str = \"\"\n",
        "    title: str = \"\"\n",
        "\n",
        "# ✅ Define Gemini LLM\n",
        "gemini_llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.7)\n",
        "\n",
        "# ✅ Generate AI-Related Quotes\n",
        "def generate_quotes(state: BlogState) -> BlogState:\n",
        "    prompt = f\"Provide 3-5 AI-related quotes relevant to the topic: {state.topic}\"\n",
        "    response = gemini_llm.invoke(prompt)\n",
        "    state.quotes = response.content if isinstance(response, AIMessage) else str(response)\n",
        "    return state\n",
        "\n",
        "# ✅ Generate Blog Content using Topic & Structure\n",
        "def write_blog(state: BlogState) -> BlogState:\n",
        "    prompt = (\n",
        "        f\"Write a detailed, structured blog post about {state.topic} using the following structure:\\n\"\n",
        "        f\"{state.structure}\\n\\n\"\n",
        "        f\"Use the following AI-related quotes to add credibility: {state.quotes}.\\n\"\n",
        "        \"Ensure it is engaging, informative, follows best SEO practices, and is between 1200-1800 words.\"\n",
        "    )\n",
        "    response = gemini_llm.invoke(prompt)\n",
        "    state.blog_content = response.content if isinstance(response, AIMessage) else str(response)\n",
        "    return state\n",
        "\n",
        "# ✅ Enhance Readability & Formatting\n",
        "def enhance_readability(state: BlogState) -> BlogState:\n",
        "    prompt = (\n",
        "        f\"Improve readability & formatting of the following blog while making it engaging:\\n\"\n",
        "        f\"{state.blog_content}\\n\\n\"\n",
        "        \"Ensure proper paragraph structuring, headings, bullet points, and optimize for SEO.\"\n",
        "    )\n",
        "    response = gemini_llm.invoke(prompt)\n",
        "    state.final_blog = response.content if isinstance(response, AIMessage) else str(response)\n",
        "    return state\n",
        "\n",
        "def generate_title(state: BlogState) -> BlogState:\n",
        "    prompt = (\n",
        "        f\"Create an eye-catching, SEO-optimized blog title for a post on {state.topic}, \"\n",
        "        \"following this structure:\\n\"\n",
        "        f\"{state.structure}\\n\\n\"\n",
        "        \"Ensure it's engaging and relevant to AI professionals.\"\n",
        "    )\n",
        "    response = gemini_llm.invoke(prompt)\n",
        "    state.title = response.content if isinstance(response, AIMessage) else str(response)\n",
        "    return state\n",
        "\n",
        "# ✅ Define Workflow Graph\n",
        "workflow = StateGraph(BlogState)\n",
        "workflow.add_node(\"quote_agent\", generate_quotes)\n",
        "workflow.add_node(\"blog_writer\", write_blog)\n",
        "workflow.add_node(\"readability_agent\", enhance_readability)\n",
        "workflow.add_node(\"title_generator\", generate_title)  # Added title generation step\n",
        "\n",
        "# ✅ Define Execution Flow\n",
        "workflow.set_entry_point(\"quote_agent\")\n",
        "workflow.add_edge(\"quote_agent\", \"blog_writer\")\n",
        "workflow.add_edge(\"blog_writer\", \"readability_agent\")\n",
        "workflow.add_edge(\"readability_agent\", \"title_generator\")  # Title generation happens last\n",
        "\n",
        "# ✅ Compile & Run Workflow\n",
        "final_workflow = workflow.compile()"
      ],
      "metadata": {
        "id": "5aH3DvsoZ9F5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = final_workflow.invoke(BlogState(topic=best_topic['query'], structure=response))"
      ],
      "metadata": {
        "id": "5CmiDTUxRBP1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title= result[\"title\"].replace(\"*\", \"\")\n",
        "content= result[\"final_blog\"].replace(\"*\", \"\")"
      ],
      "metadata": {
        "id": "XlioFwc27520"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Uploading The Blog**"
      ],
      "metadata": {
        "id": "aa63G65kRbgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from email import encoders\n",
        "from email.mime.base import MIMEBase\n",
        "import smtplib\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "\n"
      ],
      "metadata": {
        "id": "HOA3rRDmycS0"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def send_email_with_attachment(sender_email, app_password, receiver_email, subject, body):\n",
        "    try:\n",
        "        # Create a multipart message\n",
        "        msg = MIMEMultipart()\n",
        "        msg['From'] = sender_email\n",
        "        msg['To'] = receiver_email\n",
        "        msg['Subject'] = subject\n",
        "\n",
        "        # Attach the body with UTF-8 encoding\n",
        "        msg.attach(MIMEText(body, 'plain', 'utf-8'))\n",
        "\n",
        "        # Set up the SMTP server and send the email\n",
        "        server = smtplib.SMTP('smtp.gmail.com', 587)\n",
        "        server.starttls()  # Start TLS encryption\n",
        "        server.login(sender_email, app_password)\n",
        "        server.sendmail(sender_email, receiver_email, msg.as_string())\n",
        "\n",
        "        print(f\"Email sent successfully to {receiver_email}!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error sending email: {e}\")\n"
      ],
      "metadata": {
        "id": "ki3FeTm93ebg"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "send_email_with_attachment(YOUR_EMAIL,APP_PASSWORD,\"EMAIL\",title,content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCr7FMeD3qY-",
        "outputId": "f33997a4-21c3-45ab-ac2f-67ec68820bac"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Email sent successfully to vexi413qefa@post.wordpress.com!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "axyHivXYRr_u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}